{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yC9DhLUjHx4I"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('twitter30k.csv')"
      ],
      "metadata": {
        "id": "DaCRw727H8TO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['twitts'].tolist()\n",
        "y = df['sentiment']"
      ],
      "metadata": {
        "id": "Y8dqCd-xIady"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "9B2tfOiVIemt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03da62bd",
        "outputId": "a341582b-1044-481b-b9b2-df771a851be6"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "i1q2LFocJdnX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6ee3736",
        "outputId": "53ad7481-2160-4da5-8d30-39f5aeb140db"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "for sent in X:\n",
        "    sent = re.sub(\"[^a-zA-Z]\", \" \", sent)\n",
        "    sent = sent.lower().split()\n",
        "    sent = [lemmatizer.lemmatize(word) for word in sent if word not in set(stop_words)]\n",
        "    sent = \" \".join(sent)\n",
        "    x_train.append(sent)"
      ],
      "metadata": {
        "id": "wvF7LdqkJx3Y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)"
      ],
      "metadata": {
        "id": "EmVFBkVyJys0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "RRbhSfSzJ4ZJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts = tokenizer.texts_to_sequences(x_train)"
      ],
      "metadata": {
        "id": "5cx-U8PuJ8J7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_list = []\n",
        "for l in x_train:\n",
        "    testing_list.append(len(l.split(' ')))\n",
        "max_sentence_length = max(testing_list)"
      ],
      "metadata": {
        "id": "aPho64l6KAYx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_X = pad_sequences(encoded_texts, maxlen=max_sentence_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "lu8dcGpGKEXs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(padded_X, y, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "WBDof6k-KILU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_features = 300"
      ],
      "metadata": {
        "id": "yl5q0MTbKL7m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_features, input_length=max_sentence_length))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(64, recurrent_activation='relu', recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(LSTM(32, recurrent_activation='relu', recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "josGGcQfKPFd",
        "outputId": "e79d56d2-ee50-446a-84e4-1c66e5d0a213"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9Pm63P9mKSmt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nifIXpwHKXZo",
        "outputId": "26b39f57-e085-4f61-cd70-54e1557ab146"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 25ms/step - accuracy: 0.4950 - loss: 0.6932 - val_accuracy: 0.5571 - val_loss: 0.6867\n",
            "Epoch 2/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 16ms/step - accuracy: 0.5917 - loss: 0.6693 - val_accuracy: 0.6300 - val_loss: 0.6477\n",
            "Epoch 3/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.6520 - loss: 0.6273 - val_accuracy: 0.6353 - val_loss: 0.6318\n",
            "Epoch 4/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.6647 - loss: 0.6258 - val_accuracy: 0.6367 - val_loss: 0.6397\n",
            "Epoch 5/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.6600 - loss: 0.6216 - val_accuracy: 0.6342 - val_loss: 0.6353\n",
            "Epoch 6/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.6728 - loss: 0.6028 - val_accuracy: 0.6438 - val_loss: 0.6323\n",
            "Epoch 7/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.6867 - loss: 0.5912 - val_accuracy: 0.6544 - val_loss: 0.6234\n",
            "Epoch 8/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.7037 - loss: 0.5759 - val_accuracy: 0.6520 - val_loss: 0.6335\n",
            "Epoch 9/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.6985 - loss: 0.5810 - val_accuracy: 0.6538 - val_loss: 0.6320\n",
            "Epoch 10/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7134 - loss: 0.5676 - val_accuracy: 0.6613 - val_loss: 0.6275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"sentiment_lstm_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF5JQCV3KbpY",
        "outputId": "bfdc81ab-35bc-43cf-e838-c105b934f11d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "lstm_model = load_model('sentiment_lstm_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0j5M_gcg9lJ",
        "outputId": "46e10448-90c6-41fa-8075-1becf8939a5d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiments = ['negative', 'positive']"
      ],
      "metadata": {
        "id": "LHa7mIEPhDyV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocess(text):\n",
        "    encoded = tokenizer.texts_to_sequences(text)\n",
        "    padded = pad_sequences(encoded, maxlen=max_sentence_length, padding='post', truncating='post')\n",
        "    return padded"
      ],
      "metadata": {
        "id": "wz6xFY8shHBn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet1 = ['i want to kill myself'] #neg sentiment 0\n",
        "tweet2 = ['thank you very much'] #pos sentiment 1\n",
        "txt1 = text_preprocess(tweet1)\n",
        "txt2 = text_preprocess(tweet2)\n",
        "output1 = sentiments[(lstm_model.predict(txt1) > 0.5).astype(\"int32\")[0][0]]\n",
        "output2 = sentiments[(lstm_model.predict(txt2) > 0.5).astype(\"int32\")[0][0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlR5Kg6phKnD",
        "outputId": "2150b77b-3f2c-4d59-91d0-b554f30098b5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output1, output2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ICgXLQhhOBv",
        "outputId": "8dd14194-0884-408c-fd20-6accff0e6800"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative positive\n"
          ]
        }
      ]
    }
  ]
}