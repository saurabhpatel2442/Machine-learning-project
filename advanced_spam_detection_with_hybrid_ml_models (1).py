# -*- coding: utf-8 -*-
"""Advanced Spam Detection with Hybrid ML Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pifFv-gxN0O91bKvk0MEeEF4_a6JjTR5
"""

# 1. IMPORTING NECESSARY LIBRARIES
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
from wordcloud import WordCloud

# 2. DATA LOADING AND INITIAL EXPLORATION

    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

# Load the dataset from the local CSV file
df = pd.read_csv('spam_ham_dataset.csv')
print("Data Analysis")
print("Dataset Head:")
print(df.head())
print("\nDataset Info:")
df.info()
print("\nValue Counts of Labels:")
print(df['label'].value_counts())
print("-" * 30)
df = df.drop('label_num', axis=1)
df.rename(columns={'Unnamed: 0': 'id'}, inplace=True)


# 3. DATA VISUALIZATION
plt.figure(figsize=(8, 6))
sns.countplot(x='label', data=df, palette='viridis')
plt.title('Distribution of Spam vs. Ham Messages')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# 4. TEXT PREPROCESSING
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r'http\S+', '', text)
    text = re.sub('[^a-zA-Z]', ' ', text)
    text = text.lower()
    # Tokenize
    words = text.split()
    # Lemmatize and remove stop words
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return " ".join(lemmatized_words)

print("--- Preprocessing Text Data ---")
df['processed_text'] = df['text'].apply(preprocess_text)
print("Preprocessing complete.")
print("Sample of processed text:")
print(df[['text', 'processed_text']].head())
print("-" * 30)


# 5. FEATURE EXTRACTION (TF-IDF)
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X = tfidf_vectorizer.fit_transform(df['processed_text']).toarray()
y = df['label'].apply(lambda x: 1 if x == 'spam' else 0)
spam_words = " ".join(df[df['label'] == 'spam']['processed_text'])
ham_words = " ".join(df[df['label'] == 'ham']['processed_text'])

spam_wordcloud = WordCloud(width=800, height=400, background_color='black').generate(spam_words)
ham_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(ham_words)

plt.figure(figsize=(20, 10))
plt.subplot(1, 2, 1)
plt.imshow(spam_wordcloud, interpolation='bilinear')
plt.title('Most Frequent Words in Spam Messages', fontsize=16)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(ham_wordcloud, interpolation='bilinear')
plt.title('Most Frequent Words in Ham Messages', fontsize=16)
plt.axis('off')
plt.show()


# 6. MODEL TRAINING & EVALUATION
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
nb_model = MultinomialNB()
svm_model = SVC(kernel='linear', probability=True, random_state=42)
dt_model = DecisionTreeClassifier(random_state=42)
hybrid_model = VotingClassifier(
    estimators=[('nb', nb_model), ('svm', svm_model), ('dt', dt_model)],
    voting='soft'
)

# Dictionary to hold models and their results
models = {
    "Naive Bayes": nb_model,
    "SVM": svm_model,
    "Decision Tree": dt_model,
    "Hybrid Model": hybrid_model
}

results = {}

#  Train and evaluate each model
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    report = classification_report(y_test, y_pred, target_names=['Ham', 'Spam'])
    cm = confusion_matrix(y_test, y_pred)

    results[name] = {'accuracy': accuracy, 'f1_score': f1, 'report': report, 'cm': cm}
    print(f"{name} training complete.")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  F1-Score: {f1:.4f}\n")
print("-" * 30)


# 7. RESULTS VISUALIZATION
f1_scores = {name: res['f1_score'] for name, res in results.items()}
accuracy_scores = {name: res['accuracy'] for name, res in results.items()}

fig, ax = plt.subplots(1, 2, figsize=(16, 6))

# F1-Score Comparison
sns.barplot(x=list(f1_scores.keys()), y=list(f1_scores.values()), ax=ax[0], palette='magma')
ax[0].set_title('Model F1-Score Comparison')
ax[0].set_ylabel('F1-Score')
ax[0].set_ylim(0.9, 1.0)

# Accuracy Comparison
sns.barplot(x=list(accuracy_scores.keys()), y=list(accuracy_scores.values()), ax=ax[1], palette='plasma')
ax[1].set_title('Model Accuracy Comparison')
ax[1].set_ylabel('Accuracy')
ax[1].set_ylim(0.9, 1.0)

plt.tight_layout()
plt.show()

# Display Classification Reports and Confusion Matrices
for name, result in results.items():
    print(f"--- Classification Report for {name} ---")
    print(result['report'])
    print("-" * 30)

    # Plot Confusion Matrix
    plt.figure(figsize=(6, 5))
    sns.heatmap(result['cm'], annot=True, fmt='d', cmap='Blues',
                xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])
    plt.title(f'Confusion Matrix for {name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

# Detailed look at the best model (Hybrid)
print("\n--- Detailed Analysis of the Hybrid Model ---")
print("The Hybrid Model, using soft voting, combines the strengths of Naive Bayes, SVM, and Decision Tree.")
print("This ensemble approach often leads to better generalization and more robust performance, as demonstrated by its high accuracy and F1-score.")
print(f"Final Hybrid Model Accuracy: {results['Hybrid Model']['accuracy']:.4f}")

# Example Prediction
def predict_spam(text):
    """Predicts if a given text is spam or ham."""
    processed = preprocess_text(text)
    vectorized = tfidf_vectorizer.transform([processed]).toarray() # Convert to dense array
    prediction = hybrid_model.predict(vectorized)
    probability = hybrid_model.predict_proba(vectorized)

    if prediction[0] == 1:
        return f"Spam (Confidence: {probability[0][1]:.2%})"
    else:
        return f"Ham (Confidence: {probability[0][0]:.2%})"

# Example usage:
test_email_1 = "Subject: Congratulations! You've won a $1000 Walmart gift card. Click here to claim."
test_email_2 = "Subject: Project Update - Meeting moved to 3pm tomorrow in conference room 4."
test_email_3 = "Subject: Exclusive deal just for you! Limited time offer on viagra, get it now!"

print("\n--- Testing with Example Emails ---")
print(f"Email 1: '{test_email_1}'")
print(f"Prediction: {predict_spam(test_email_1)}\n")

print(f"Email 2: '{test_email_2}'")
print(f"Prediction: {predict_spam(test_email_2)}\n")

print(f"Email 3: '{test_email_3}'")
print(f"Prediction: {predict_spam(test_email_3)}\n")